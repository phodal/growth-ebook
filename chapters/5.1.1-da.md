数据分析
---

数据分析是一个很有意思的过程，我们可以简单地将这个过程分成四个步骤：

 - 识别需求
 - 收集数据
 - 分析数据
 - 展示数据

值得注意的是：在分析数据的过程中，需要不同的人员来参与，需要跨域多个领域的知识点——分析、设计、开发、商业和研究等领域。因此，在这样的领域里，回归敏捷也是一种不错的选择（源于：《敏捷数据科学》）：

 - 通才高于专长
 - 小团队高于大团队
 - 使用高阶工具和平台：云计算、分布式系统、PaaS
 - 持续、迭代地分享工作成果，即使这些工作未完成

###识别需求

在我们开始分析数据之前，我们需要明确一下，我们的问题是什么？即，我们到底要干嘛，我们想要的内容是什么。

> 识别信息需求是确保数据分析过程有效性的首要条件，可以为收集数据、分析数据提供清晰的目标。

当我们想要提到我们的网站在不同的地区的速度时，我们就需要去探索我们的用户主要是在哪些地区。即，现在这是我们的需求。我们已经有了这样的一个明确的目标，下面要做起来就很轻松了。

###收集数据

那么现在新的问题来了，我们的数据要从哪里来？

对于大部分的网站来说，都会有访问日志。但是这些访问日志只能显示某个IP进入了某个页面，并不人详细地介绍这个用户在这个页面待了多久，做了什么事。这时候，这些数据就需要依赖于类似于Google Analytics这样的工具来统计网站的流量。还有类似于New Relic这样的工具来统计用户的一些行为。

在一些以科学研究为目的的数据收集中，我们可以从一些公开的数据中获取这些资料。

而在一些特殊的情况里，我们就需要通过爬虫来完成这样的工作。

###分析数据

现在，我们终于可以真正的去分析数据了——我的意思是，我们要开始写代码了。从海量的数据中过滤出我们想要的数据，并通过算法来对其进行分析。

一般来说，我们都利用现有的工具来完成大部分的工作。要使用哪一类工具，取决于我们如要分析的数据的数量级了。如果只是一般的数量级，我们可以考虑用R语言、Python、Octave等单机工具来完成。如果是大量的数据，那么我们就需要考虑用Hadoop、Spark来完成这个级别的工作。

而一般来说，这个过程可能是要经过一系列的工具才能完成。如在之前我在分析我的博客的日志时(1G左右)，我用Hadoop + Apache Pig + Jython来将日志中的IP转换为GEO信息，再将GEO信息存储到ElasticSearch中。随后，我们就可以用AMap、leaflet这一类GEO库将这些点放置到地图上。

###展示数据



参考来源: **精益数据分析**。
